[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "llama-lime",
    "section": "",
    "text": "llama-lime is a cutting-edge toolkit designed for data scientists, machine learning practitioners, researchers, and AI enthusiasts. It provides powerful tools to analyze, explore, and explain complex models, including Large Language Models (LLMs), with transparency and ease."
  },
  {
    "objectID": "index.html#welcome-to-llama-lime-leveraging-large-models-for-analysis-and-local-interpretable-model-explanations",
    "href": "index.html#welcome-to-llama-lime-leveraging-large-models-for-analysis-and-local-interpretable-model-explanations",
    "title": "llama-lime",
    "section": "",
    "text": "llama-lime is a cutting-edge toolkit designed for data scientists, machine learning practitioners, researchers, and AI enthusiasts. It provides powerful tools to analyze, explore, and explain complex models, including Large Language Models (LLMs), with transparency and ease."
  },
  {
    "objectID": "index.html#what-we-offer",
    "href": "index.html#what-we-offer",
    "title": "llama-lime",
    "section": "üéØ What We Offer",
    "text": "üéØ What We Offer\nExploration: Dive into your data with our Explorer class, equipped with comprehensive tools for Exploratory Data Analysis (EDA). Understand your data‚Äôs underlying patterns, distributions, and relationships like never before. Explanation: Demystify AI with our Explainer class, offering robust explainability techniques such as SHAP and LIME. From text to vision, we support various domains and tasks."
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "llama-lime",
    "section": "üåü Features",
    "text": "üåü Features\nVersatility: Seamlessly integrate llama-lime with diverse models, including Neural Networks, Random Forests, Hugging Face Transformers, and more. Multimodal Support: Analyze and explain models across a wide range of tasks, including classification, object detection, summarization, and more. User-Centric: Enjoy llama-lime‚Äôs user-friendly and intuitive APIs, designed for both beginners and experts alike. Community-Driven: We value collaboration and innovation. Contribute, learn, and grow with the llama-lime community."
  },
  {
    "objectID": "index.html#how-to-get-started",
    "href": "index.html#how-to-get-started",
    "title": "llama-lime",
    "section": "üõ†Ô∏è How to Get Started",
    "text": "üõ†Ô∏è How to Get Started\nReady to explore the world of explainable AI? Check out our Getting Started Guide, Tutorials, and API Documentation to embark on your journey with llama-lime."
  },
  {
    "objectID": "index.html#join-the-community",
    "href": "index.html#join-the-community",
    "title": "llama-lime",
    "section": "ü§ù Join the Community",
    "text": "ü§ù Join the Community\nWe welcome contributions and collaboration. Whether you‚Äôre a seasoned professional or just starting in AI, there‚Äôs room for you in the llama-lime family. Visit our GitHub to connect, contribute, and innovate.\nüéâ Unlock the potential of AI with llama-lime. Explore, Explain, and Empower your models with transparency and confidence!"
  },
  {
    "objectID": "random_forest_explanation.html",
    "href": "random_forest_explanation.html",
    "title": "Explaining a Random Forest",
    "section": "",
    "text": "We‚Äôll start with a super easy example, the obligatory iris dataset with a random forest classifier. Below we set up the model using scikit-learn, everything so far should look very familiar.\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_iris\n\nnp.random.seed(123)\n\niris = load_iris()\nrandom_forest = RandomForestClassifier()\nSince we‚Äôll use GPT-4 for the tutorial, and because GPT-4 is very smart, it probably knows the iris dataset anyway. To trick it a little bit into thinking we have a new dataset, let‚Äôs just rename the features and ‚Äújumble‚Äù the data a bit.\nfeature_names = [\"length of table\", \"width of table\", \"length of dresser\", \"width of dresser\"]\nclass_names = [\"living room\", \"bedroom\", \"dining room\"]\n\nX = iris.data * 1.8182\ny = iris.target\nrandom_forest.fit(X, y)\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier()"
  },
  {
    "objectID": "random_forest_explanation.html#explaining-the-model",
    "href": "random_forest_explanation.html#explaining-the-model",
    "title": "Explaining a Random Forest",
    "section": "Explaining the Model",
    "text": "Explaining the Model\nFrom here, we‚Äôve fit the model and we can run predictions against the model with new data.\n\nimport numpy as np\n\nnew_observation = np.random.rand(1, 4)\n\nnew_observation\n\narray([[0.12062867, 0.8263408 , 0.60306013, 0.54506801]])\n\n\n\nrandom_forest.predict_proba(new_observation)\n\narray([[0.96, 0.04, 0.  ]])\n\n\nNow we can say that the most likely class to which our new observation belongs is living room. Of course, from here, we have a lot of questions. And if we don‚Äôt then the people we show our models to sure will üòÖ.\nThe random forest in scikit has some nice utilities for helping to diagnose what our model is doing under the hood. We can get the feature importances from the model.\n\nrandom_forest.feature_importances_\n\narray([0.08738947, 0.0257446 , 0.4611169 , 0.42574903])\n\n\nWe can even look at the decision path our new data took to reach it‚Äôs prediction.\n\nrandom_forest.decision_path(new_observation)\n\n(&lt;1x1708 sparse matrix of type '&lt;class 'numpy.int64'&gt;'\n    with 226 stored elements in Compressed Sparse Row format&gt;,\n array([   0,   17,   38,   59,   82,   97,  110,  123,  138,  157,  170,\n         183,  206,  225,  246,  269,  298,  323,  330,  347,  358,  375,\n         388,  407,  422,  447,  474,  493,  510,  525,  542,  549,  562,\n         581,  598,  611,  624,  641,  656,  675,  700,  715,  724,  747,\n         764,  777,  790,  803,  818,  835,  858,  873,  890,  911,  934,\n         949,  962,  977,  990, 1003, 1028, 1041, 1060, 1071, 1094, 1107,\n        1120, 1135, 1156, 1167, 1186, 1201, 1216, 1239, 1258, 1271, 1278,\n        1299, 1318, 1341, 1362, 1391, 1408, 1427, 1444, 1457, 1472, 1493,\n        1512, 1525, 1538, 1553, 1574, 1597, 1620, 1635, 1652, 1669, 1680,\n        1691, 1708]))\n\n\nGreat! We‚Äôre done!\n\nNot So Fast‚Ä¶\nSo, at this point there are a couple of issues.\n1.) What the heck does any of that mean? I‚Äôm a data scientist, and even I don‚Äôt think any of this output is useful. 2.) This works for a random forest. Does it work for anything else? Can I use it in any scikit model? What about outside of scikit? 3.) How do I make this make sense for a variety of different audiences? How do I explain it to my boss? The woman in accounting I‚Äôm building the model for? My mom?\nLet‚Äôs start with #2. There have been many advances in explainable machine learning in recent years, and a big focus has been on creating model-agnostic explainers. Two of the biggest ones are SHAP](https://shap.readthedocs.io/en/latest/tabular_examples.html) and LIME.\nSo, if we use model-agnostic explainers what do we get?\n\nimport shap\n\nshap_sample = shap.utils.sample(X, 10)\n\nexplainer = shap.Explainer(random_forest.predict, shap_sample)\nshap_values = explainer(X)\n\nIProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n\n\nThen we can even print out a nice plot!\n\nshap.plots.waterfall(shap_values[8], max_display=14)\n\n\n\n\nThis is great, but it‚Äôs still missing something. You still need to explain to a layperson what they‚Äôre looking at. This might help solve issue 1.) from above, but issue 3.) remains. We can make sense of the SHAP scores if they are displayed to us nicely, but we still need to know what the SHAP score is doing. Can we do better?"
  },
  {
    "objectID": "random_forest_explanation.html#creating-an-explainer-that-explains",
    "href": "random_forest_explanation.html#creating-an-explainer-that-explains",
    "title": "Explaining a Random Forest",
    "section": "Creating an Explainer that Explains",
    "text": "Creating an Explainer that Explains\nSHAP is great; we don‚Äôt want to redo the wheel. What we want to do is solve issue 3.) from above. To do so, let‚Äôs wrap up everything we have in a simple class, similar to the Explainer in the shap library.\n\nimport os\nimport sys\nimport inspect\n\ncurrentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\nparentdir = os.path.dirname(currentdir)\nsys.path.insert(0, parentdir) \n\n\nfrom llama_lime.explainer import Explainer\nexplainer = Explainer(\n    random_forest,\n    features=X,\n    feature_names=feature_names,\n    class_names=class_names,\n    output=y,\n    target_audience=\"a business person with a solid understanding of basic statistics\"\n    )\n\nYou might see that target_audience is something new. This takes in a natural-language prompt explaining\nJust like before we can print out the SHAP value.\nexplainer.add_feature_importances() explainer.add_class_importances() explainer.add_instance_importances() explainer.add_feature_class_interactions() explainer.add_lime_instances(X[10:30]) # Use the explainer to generate explanations # explanations = explainer.explain(X) explanations = explainer.explain()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]