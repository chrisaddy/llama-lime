[
  {
    "objectID": "exploring-data.html",
    "href": "exploring-data.html",
    "title": "Exploring Tabular Datasets",
    "section": "",
    "text": "A pretty common task in data science is exploratory data analysis. Let‚Äôs do that. We‚Äôll use the breast cancer dataset from scikit-learn.\nfrom sklearn import datasets\nimport pandas as pd\n\ndef load_cancer_dataset():\n    dataset = datasets.load_breast_cancer()\n    dataframe = pd.DataFrame(dataset[\"data\"])\n    dataframe.columns = dataset[\"feature_names\"]\n    dataframe[\"status\"] = dataset.target\n    dataframe[\"status\"] = dataframe[\"status\"].replace({0: \"malignant\", 1: \"benign\"})\n\n    return dataframe\n\ncancer = load_cancer_dataset()\n\ncancer\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\nstatus\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.30010\n0.14710\n0.2419\n0.07871\n...\n17.33\n184.60\n2019.0\n0.16220\n0.66560\n0.7119\n0.2654\n0.4601\n0.11890\nmalignant\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.08690\n0.07017\n0.1812\n0.05667\n...\n23.41\n158.80\n1956.0\n0.12380\n0.18660\n0.2416\n0.1860\n0.2750\n0.08902\nmalignant\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.19740\n0.12790\n0.2069\n0.05999\n...\n25.53\n152.50\n1709.0\n0.14440\n0.42450\n0.4504\n0.2430\n0.3613\n0.08758\nmalignant\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.24140\n0.10520\n0.2597\n0.09744\n...\n26.50\n98.87\n567.7\n0.20980\n0.86630\n0.6869\n0.2575\n0.6638\n0.17300\nmalignant\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.19800\n0.10430\n0.1809\n0.05883\n...\n16.67\n152.20\n1575.0\n0.13740\n0.20500\n0.4000\n0.1625\n0.2364\n0.07678\nmalignant\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n564\n21.56\n22.39\n142.00\n1479.0\n0.11100\n0.11590\n0.24390\n0.13890\n0.1726\n0.05623\n...\n26.40\n166.10\n2027.0\n0.14100\n0.21130\n0.4107\n0.2216\n0.2060\n0.07115\nmalignant\n\n\n565\n20.13\n28.25\n131.20\n1261.0\n0.09780\n0.10340\n0.14400\n0.09791\n0.1752\n0.05533\n...\n38.25\n155.00\n1731.0\n0.11660\n0.19220\n0.3215\n0.1628\n0.2572\n0.06637\nmalignant\n\n\n566\n16.60\n28.08\n108.30\n858.1\n0.08455\n0.10230\n0.09251\n0.05302\n0.1590\n0.05648\n...\n34.12\n126.70\n1124.0\n0.11390\n0.30940\n0.3403\n0.1418\n0.2218\n0.07820\nmalignant\n\n\n567\n20.60\n29.33\n140.10\n1265.0\n0.11780\n0.27700\n0.35140\n0.15200\n0.2397\n0.07016\n...\n39.42\n184.60\n1821.0\n0.16500\n0.86810\n0.9387\n0.2650\n0.4087\n0.12400\nmalignant\n\n\n568\n7.76\n24.54\n47.92\n181.0\n0.05263\n0.04362\n0.00000\n0.00000\n0.1587\n0.05884\n...\n30.37\n59.16\n268.6\n0.08996\n0.06444\n0.0000\n0.0000\n0.2871\n0.07039\nbenign\n\n\n\n\n569 rows √ó 31 columns\nNow, it‚Äôs as simple as‚Ä¶\nfrom llama_lime.explorer import Explorer\n\nexplorer = Explorer(cancer)\n\n/Users/chrisaddy/Library/Caches/pypoetry/virtualenvs/llama-lime-Ipq4cX_Y-py3.11/lib/python3.11/site-packages/numba/core/decorators.py:262: NumbaDeprecationWarning: numba.generated_jit is deprecated. Please see the documentation at: https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-generated-jit for more information and advice on a suitable replacement.\n  warnings.warn(msg, NumbaDeprecationWarning)\n/Users/chrisaddy/Library/Caches/pypoetry/virtualenvs/llama-lime-Ipq4cX_Y-py3.11/lib/python3.11/site-packages/visions/backends/shared/nan_handling.py:50: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  @nb.jit\n\n\n2023-08-18 11:38:51,867 INFO worker.py:1621 -- Started a local Ray instance.\nexplorer.explore(\"What column has the highest mean?\")\n\n{'question': 'What column has the highest mean?',\n 'chat_history': [HumanMessage(content='What column has the highest mean?', additional_kwargs={}, example=False),\n  AIMessage(content='The column with the highest mean is \"mean area\".', additional_kwargs={}, example=False)],\n 'answer': 'The column with the highest mean is \"mean area\".'}"
  },
  {
    "objectID": "exploring-data.html#back-and-forth-chatting",
    "href": "exploring-data.html#back-and-forth-chatting",
    "title": "Exploring Tabular Datasets",
    "section": "Back and Forth Chatting",
    "text": "Back and Forth Chatting\nThe Explorer, under the hood, has a memory buffer which we can use to have more natural back and forth chats.\nWithout the buffer, if we asked what is the minimum value of that column? after our initial prompt, it wouldn‚Äôt know to what that column refers. Now, since it has a memory buffer, it knows we are talking about the column with the highest mean.\n\nexplorer.explore(\"What is the minimum of that column\") \n\n{'question': 'What is the minimum of that column',\n 'chat_history': [HumanMessage(content='What column has the highest mean?', additional_kwargs={}, example=False),\n  AIMessage(content='The column with the highest mean is \"mean area\".', additional_kwargs={}, example=False),\n  HumanMessage(content='What is the minimum of that column', additional_kwargs={}, example=False),\n  AIMessage(content='The minimum value in the \"mean area\" column is 143.5.', additional_kwargs={}, example=False)],\n 'answer': 'The minimum value in the \"mean area\" column is 143.5.'}"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "llama-lime",
    "section": "",
    "text": "llama-lime is a cutting-edge toolkit designed for data scientists, machine learning practitioners, researchers, and AI tinkerers. It provides powerful tools to analyze, explore, and explain complex models, including Large Language Models (LLMs), with transparency and ease."
  },
  {
    "objectID": "index.html#welcome-to-llama-lime-leveraging-large-models-for-analysis-and-local-interpretable-model-explanations",
    "href": "index.html#welcome-to-llama-lime-leveraging-large-models-for-analysis-and-local-interpretable-model-explanations",
    "title": "llama-lime",
    "section": "",
    "text": "llama-lime is a cutting-edge toolkit designed for data scientists, machine learning practitioners, researchers, and AI tinkerers. It provides powerful tools to analyze, explore, and explain complex models, including Large Language Models (LLMs), with transparency and ease."
  },
  {
    "objectID": "index.html#what-we-offer",
    "href": "index.html#what-we-offer",
    "title": "llama-lime",
    "section": "üéØ What We Offer",
    "text": "üéØ What We Offer\nExploration: Dive into your data with our Explorer class, equipped with comprehensive tools for Exploratory Data Analysis (EDA). Understand your data‚Äôs underlying patterns, distributions, and relationships like never before.\nExplanation: Demystify AI with our Explainer class, offering robust explainability techniques such as SHAP and LIME. From text to vision, we support various domains and tasks."
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "llama-lime",
    "section": "üåü Features",
    "text": "üåü Features\nVersatility: Seamlessly integrate llama-lime with diverse models, including Neural Networks, Random Forests, Hugging Face Transformers, and more.\nMultimodal Support: Analyze and explain models across a wide range of tasks, including classification, object detection, summarization, and more.\nUser-Centric: Enjoy llama-lime‚Äôs user-friendly and intuitive APIs, designed for both beginners and experts alike.\nCommunity-Driven: We value collaboration and innovation. Contribute, learn, and grow with the llama-lime community."
  },
  {
    "objectID": "index.html#how-to-get-started",
    "href": "index.html#how-to-get-started",
    "title": "llama-lime",
    "section": "üõ†Ô∏è How to Get Started",
    "text": "üõ†Ô∏è How to Get Started\nReady to explore the world of explainable AI? Check out our Getting Started Guide, Tutorials, and API Documentation to embark on your journey with llama-lime."
  },
  {
    "objectID": "index.html#join-the-community",
    "href": "index.html#join-the-community",
    "title": "llama-lime",
    "section": "ü§ù Join the Community",
    "text": "ü§ù Join the Community\nWe welcome contributions and collaboration. Whether you‚Äôre a seasoned professional or just starting in AI, there‚Äôs room for you in the llama-lime family. Visit our GitHub to connect, contribute, and innovate.\nüéâ Unlock the potential of AI with llama-lime. Explore, Explain, and Empower your models with transparency and confidence!"
  },
  {
    "objectID": "random_forest_explanation.html",
    "href": "random_forest_explanation.html",
    "title": "Explaining a Random Forest",
    "section": "",
    "text": "We‚Äôll start with a super easy example, the obligatory iris dataset with a random forest classifier. Below we set up the model using scikit-learn, everything so far should look very familiar.\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_iris\n\nnp.random.seed(123)\n\niris = load_iris()\nrandom_forest = RandomForestClassifier()\nSince we‚Äôll use GPT-4 for the tutorial, and because GPT-4 is very smart, it probably knows the iris dataset anyway. To trick it a little bit into thinking we have a new dataset, let‚Äôs just rename the features and ‚Äújumble‚Äù the data a bit.\nfeature_names = [\"length of table\", \"width of table\", \"length of dresser\", \"width of dresser\"]\nclass_names = [\"living room\", \"bedroom\", \"dining room\"]\n\nX = iris.data * 1.8182\ny = iris.target\nrandom_forest.fit(X, y)\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier()"
  },
  {
    "objectID": "random_forest_explanation.html#explaining-the-model",
    "href": "random_forest_explanation.html#explaining-the-model",
    "title": "Explaining a Random Forest",
    "section": "Explaining the Model",
    "text": "Explaining the Model\nFrom here, we‚Äôve fit the model and we can run predictions against the model with new data.\n\nimport numpy as np\n\nnew_observation = np.random.rand(1, 4)\n\nnew_observation\n\narray([[0.12062867, 0.8263408 , 0.60306013, 0.54506801]])\n\n\n\nrandom_forest.predict_proba(new_observation)\n\narray([[0.96, 0.04, 0.  ]])\n\n\nNow we can say that the most likely class to which our new observation belongs is living room. Of course, from here, we have a lot of questions. And if we don‚Äôt then the people we show our models to sure will üòÖ.\nThe random forest in scikit has some nice utilities for helping to diagnose what our model is doing under the hood. We can get the feature importances from the model.\n\nrandom_forest.feature_importances_\n\narray([0.08738947, 0.0257446 , 0.4611169 , 0.42574903])\n\n\nWe can even look at the decision path our new data took to reach it‚Äôs prediction.\n\nrandom_forest.decision_path(new_observation)\n\n(&lt;1x1708 sparse matrix of type '&lt;class 'numpy.int64'&gt;'\n    with 226 stored elements in Compressed Sparse Row format&gt;,\n array([   0,   17,   38,   59,   82,   97,  110,  123,  138,  157,  170,\n         183,  206,  225,  246,  269,  298,  323,  330,  347,  358,  375,\n         388,  407,  422,  447,  474,  493,  510,  525,  542,  549,  562,\n         581,  598,  611,  624,  641,  656,  675,  700,  715,  724,  747,\n         764,  777,  790,  803,  818,  835,  858,  873,  890,  911,  934,\n         949,  962,  977,  990, 1003, 1028, 1041, 1060, 1071, 1094, 1107,\n        1120, 1135, 1156, 1167, 1186, 1201, 1216, 1239, 1258, 1271, 1278,\n        1299, 1318, 1341, 1362, 1391, 1408, 1427, 1444, 1457, 1472, 1493,\n        1512, 1525, 1538, 1553, 1574, 1597, 1620, 1635, 1652, 1669, 1680,\n        1691, 1708]))\n\n\nGreat! We‚Äôre done!\n\nNot So Fast‚Ä¶\nSo, at this point there are a couple of issues.\n1.) What the heck does any of that mean? I‚Äôm a data scientist, and even I don‚Äôt think any of this output is useful. 2.) This works for a random forest. Does it work for anything else? Can I use it in any scikit model? What about outside of scikit? 3.) How do I make this make sense for a variety of different audiences? How do I explain it to my boss? The woman in accounting I‚Äôm building the model for? My mom?\nLet‚Äôs start with #2. There have been many advances in explainable machine learning in recent years, and a big focus has been on creating model-agnostic explainers. Two of the biggest ones are SHAP](https://shap.readthedocs.io/en/latest/tabular_examples.html) and LIME.\nSo, if we use model-agnostic explainers what do we get?\n\nimport shap\n\nshap_sample = shap.utils.sample(X, 10)\n\nexplainer = shap.Explainer(random_forest.predict, shap_sample)\nshap_values = explainer(X)\n\nUsing `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n\n\nThen we can even print out a nice plot!\n\nshap.plots.waterfall(shap_values[8], max_display=14)\n\n\n\n\nThis is great, but it‚Äôs still missing something. You still need to explain to a layperson what they‚Äôre looking at. This might help solve issue 1.) from above, but issue 3.) remains. We can make sense of the SHAP scores if they are displayed to us nicely, but we still need to know what the SHAP score is doing. Can we do better?"
  },
  {
    "objectID": "random_forest_explanation.html#creating-an-explainer-that-explains",
    "href": "random_forest_explanation.html#creating-an-explainer-that-explains",
    "title": "Explaining a Random Forest",
    "section": "Creating an Explainer that Explains",
    "text": "Creating an Explainer that Explains\nSHAP is great; we don‚Äôt want to redo the wheel. What we want to do is solve issue 3.) from above. To do so, let‚Äôs wrap up everything we have in a simple class, similar to the Explainer in the shap library.\n\nfrom llama_lime.explainer import Explainer\n\nexplainer = Explainer(\n    random_forest,\n    features=X,\n    feature_names=feature_names,\n    class_names=class_names,\n    output=y,\n    target_audience=\"a business person with a solid understanding of basic statistics\"\n    )\n\nYou might see that target_audience is something new. This takes in a natural-language prompt explaining\nJust like before we can print out the SHAP value.\n\nexplainer.add_shap(shap_sample)\n\nNow we HIT THE EXPLAIN BUTTON!.\n\nexplainer.explain()\n\nBased on the SHAP analysis, we can provide a detailed summary of the findings for the machine learning model. However, it seems that there are no specific results or findings available for this model. It is possible that the model did not provide any SHAP values or that the SHAP analysis was not conducted for this particular model.\n\nWithout any specific results, it is difficult to provide a detailed summary of the findings. It is important to note that SHAP values are useful in understanding the contribution of each feature towards the model's predictions. They provide insights into how each feature affects the model's output and can help in interpreting the model's decision-making process.\n\nIn the absence of specific findings, it is recommended to explore other methods or analyses to gain a better understanding of the model's predictions. This could include examining feature importance, model performance metrics, or conducting further experiments to evaluate the model's accuracy and reliability."
  },
  {
    "objectID": "random_forest_explanation.html#but-wait-theres-more",
    "href": "random_forest_explanation.html#but-wait-theres-more",
    "title": "Explaining a Random Forest",
    "section": "But Wait, There‚Äôs More‚Ä¶",
    "text": "But Wait, There‚Äôs More‚Ä¶\nWe can add in feature importances, class importances, feature-class interactions to provide even more context.\nSince this is based on the SHAP scores, these are model-agnostic versions of these measurements.\n\nexplainer.add_feature_importances()\nexplainer.add_class_importances()\nexplainer.add_feature_class_interactions()\n\nAgain, we just run explain() over the scores we are adding.\n\nexplainer.explain()\n\nBased on the analysis of the SHAP values, feature importance, class importances, and feature-class interactions, we did not find any significant findings or insights to report. The model's predictions do not appear to be strongly influenced by any specific features, classes, or interactions between features and classes. This suggests that the model's predictions are not heavily reliant on any particular factors and are likely a result of a combination of multiple features and classes.\n\nIt is important to note that these findings are specific to the dataset and model architecture used for this analysis. Different datasets or models may yield different results. Additionally, the absence of significant findings does not necessarily imply that the model is not performing well or that it is not capturing important patterns in the data. It simply means that the model's predictions are not strongly driven by any specific features or classes in this particular analysis."
  },
  {
    "objectID": "random_forest_explanation.html#with-extra-lime",
    "href": "random_forest_explanation.html#with-extra-lime",
    "title": "Explaining a Random Forest",
    "section": "With Extra Lime",
    "text": "With Extra Lime\nWe are also baking LIME into the explainability pie. This shouldn‚Äôt be too surprising given it was the inspiration for the name of the project.\n\nexplainer.add_lime(X[10:30])\nexplainer.explain()\n\nBased on the LIME instance explainer analysis, we have obtained feature contributions for each instance. These feature contributions indicate the impact of each feature on the prediction for a specific instance.\n\nThe output consists of a list of tuples, where each tuple contains a feature and its corresponding weight in the explanation. The first element of the tuple represents a statement about the feature's value, and the second element is the weight of that feature in the model's prediction.\n\nFor example, in the first tuple, the feature \"width of dresser\" with a value less than or equal to 0.55 has a negative weight of -0.195. This suggests that a smaller width of the dresser decreases the prediction. Similarly, the feature \"length of dresser\" with a value less than or equal to 2.91 has a negative weight of -0.176, indicating that a shorter length of the dresser also decreases the prediction.\n\nIt is important to note that the weights are relative to the specific instance being explained. Therefore, these feature contributions provide insights into how each feature influenced the prediction for that particular instance.\n\nBy analyzing the feature contributions across multiple instances, we can gain a deeper understanding of how different features impact the model's predictions. This information can be valuable for business decision-making, as it allows us to identify the key factors driving the model's predictions and potentially uncover any biases or patterns in the data.\n\nHowever, without specific numerical values or a target variable, it is difficult to provide a more detailed summary of the findings."
  },
  {
    "objectID": "random_forest_explanation.html#one-last-trick",
    "href": "random_forest_explanation.html#one-last-trick",
    "title": "Explaining a Random Forest",
    "section": "One Last Trick",
    "text": "One Last Trick\nthe explain() method actually takes a query argument, which by default asks for a detailed summary of the context we‚Äôve given it.\nBut the explainer is just an LLMChain. In fact, most of this library is connecting existing methodologies to LangChain.\nSo, we are not confined to just summarizing the scores we give it. We can ask it any natural-language question.\n\nexplainer.explain(\"If we had to drop a single feature which one should it be and why?\")\n\nBased on the LIME instance explainer analysis, we can determine the feature to drop by examining the weights assigned to each feature in the explanation. The feature with the smallest absolute weight can be considered for removal, as it has the least impact on the model's prediction for the specific instance.\n\nIn the provided output, we can see a list of tuples, where each tuple represents a feature and its corresponding weight in the explanation. The second element of each tuple represents the weight of the feature.\n\nFor example, let's consider the first tuple: ('width of dresser &lt;= 0.55', -0.195). The feature \"width of dresser\" has a weight of -0.195. This negative weight suggests that a decrease in the width of the dresser would decrease the model's prediction for the specific instance.\n\nTo determine the feature to drop, we need to identify the feature with the smallest absolute weight. In this case, we would need to compare the absolute values of the weights for each feature.\n\nUnfortunately, the provided output does not include the absolute values of the weights. Without this information, we cannot determine the feature to drop based on the LIME instance explainer analysis.\n\nTo provide a recommendation on which feature to drop, we would need access to the absolute values of the weights for each feature.\n\n\n\nexplainer.explain(\"What is the most compelling story we can tell about this output?\")\n\nThe most compelling story we can tell about this output is that the width and length of the dresser, as well as the length and width of the table, have a significant impact on the model's predictions. \n\nThe feature importance analysis shows that the width of the dresser and the length of the dresser are consistently negatively correlated with the predictions. This suggests that smaller widths and lengths of the dresser tend to decrease the model's predictions. On the other hand, the width and length of the table have a mixed impact on the predictions, with some instances showing a positive correlation and others showing a negative correlation.\n\nThe class importances analysis indicates that the width and length of the dresser have a relatively higher influence on the predictions compared to other features. This suggests that these features play a more significant role in determining the outcome of the model's predictions.\n\nThe feature-class interaction analysis reveals that the impact of the features on the predictions varies across different classes. For example, the width of the dresser has a negative influence on the predictions for most classes, while the length of the table has a positive influence on the predictions for some classes.\n\nOverall, this analysis highlights the importance of considering the dimensions of the dresser and table when making predictions. It suggests that smaller widths and lengths of the dresser tend to decrease the predictions, while the impact of the table dimensions varies depending on the class. This information can be valuable for understanding the factors that contribute to the model's predictions and making informed decisions based on these insights.\n\n\n\nexplainer.explain(\"Should we pick a better model?\")\n\nBased on the provided analysis, we cannot determine whether we should pick a better model or not. The analysis focused on explaining the predictions made by the model using different techniques such as SHAP values, feature importance, class importances, and feature-class interactions. However, no specific findings or results were provided in the analysis.\n\nTo make a decision on whether to pick a better model, we would need more information such as the performance metrics of the current model, comparison with other models, and the specific requirements and goals of the business. It is important to evaluate the model's performance in terms of accuracy, precision, recall, and other relevant metrics, and compare it with alternative models to determine if there is room for improvement.\n\nAdditionally, it is crucial to consider the specific needs and objectives of the business. If the current model is meeting the desired performance and providing satisfactory explanations for predictions, there may not be a need to switch to a different model. However, if the model is underperforming or the explanations are not sufficient, it may be worth exploring other models that could potentially provide better results.\n\nIn summary, without specific findings or results from the analysis, it is not possible to determine whether a better model should be chosen. A comprehensive evaluation of the current model's performance and a comparison with alternative models would be necessary to make an informed decision."
  },
  {
    "objectID": "random_forest_explanation.html#conclusion",
    "href": "random_forest_explanation.html#conclusion",
    "title": "Explaining a Random Forest",
    "section": "Conclusion",
    "text": "Conclusion\nThis should serve as a general overview of what you can do with llama-lime. In further tutorials, we‚Äôll take a peek under the hood to look at how we can modify the explainer."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "nbeats_example.html",
    "href": "nbeats_example.html",
    "title": "llama-lime",
    "section": "",
    "text": "https://lightning-flash.readthedocs.io/en/latest/notebooks/flash_tutorials/electricity_forecasting.html"
  }
]