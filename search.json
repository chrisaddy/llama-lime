[
  {
    "objectID": "exploring-data.html",
    "href": "exploring-data.html",
    "title": "Exploring Tabular Datasets",
    "section": "",
    "text": "A pretty common task in data science is exploratory data analysis. Let‚Äôs do that. We‚Äôll use the breast cancer dataset from scikit-learn.\nfrom sklearn import datasets\nimport pandas as pd\n\ndef load_cancer_dataset():\n    dataset = datasets.load_breast_cancer()\n    dataframe = pd.DataFrame(dataset[\"data\"])\n    dataframe.columns = dataset[\"feature_names\"]\n    dataframe[\"status\"] = dataset.target\n    dataframe[\"status\"] = dataframe[\"status\"].replace({0: \"malignant\", 1: \"benign\"})\n\n    return dataframe\n\ncancer = load_cancer_dataset()\n\ncancer\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\nstatus\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.30010\n0.14710\n0.2419\n0.07871\n...\n17.33\n184.60\n2019.0\n0.16220\n0.66560\n0.7119\n0.2654\n0.4601\n0.11890\nmalignant\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.08690\n0.07017\n0.1812\n0.05667\n...\n23.41\n158.80\n1956.0\n0.12380\n0.18660\n0.2416\n0.1860\n0.2750\n0.08902\nmalignant\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.19740\n0.12790\n0.2069\n0.05999\n...\n25.53\n152.50\n1709.0\n0.14440\n0.42450\n0.4504\n0.2430\n0.3613\n0.08758\nmalignant\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.24140\n0.10520\n0.2597\n0.09744\n...\n26.50\n98.87\n567.7\n0.20980\n0.86630\n0.6869\n0.2575\n0.6638\n0.17300\nmalignant\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.19800\n0.10430\n0.1809\n0.05883\n...\n16.67\n152.20\n1575.0\n0.13740\n0.20500\n0.4000\n0.1625\n0.2364\n0.07678\nmalignant\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n564\n21.56\n22.39\n142.00\n1479.0\n0.11100\n0.11590\n0.24390\n0.13890\n0.1726\n0.05623\n...\n26.40\n166.10\n2027.0\n0.14100\n0.21130\n0.4107\n0.2216\n0.2060\n0.07115\nmalignant\n\n\n565\n20.13\n28.25\n131.20\n1261.0\n0.09780\n0.10340\n0.14400\n0.09791\n0.1752\n0.05533\n...\n38.25\n155.00\n1731.0\n0.11660\n0.19220\n0.3215\n0.1628\n0.2572\n0.06637\nmalignant\n\n\n566\n16.60\n28.08\n108.30\n858.1\n0.08455\n0.10230\n0.09251\n0.05302\n0.1590\n0.05648\n...\n34.12\n126.70\n1124.0\n0.11390\n0.30940\n0.3403\n0.1418\n0.2218\n0.07820\nmalignant\n\n\n567\n20.60\n29.33\n140.10\n1265.0\n0.11780\n0.27700\n0.35140\n0.15200\n0.2397\n0.07016\n...\n39.42\n184.60\n1821.0\n0.16500\n0.86810\n0.9387\n0.2650\n0.4087\n0.12400\nmalignant\n\n\n568\n7.76\n24.54\n47.92\n181.0\n0.05263\n0.04362\n0.00000\n0.00000\n0.1587\n0.05884\n...\n30.37\n59.16\n268.6\n0.08996\n0.06444\n0.0000\n0.0000\n0.2871\n0.07039\nbenign\n\n\n\n\n569 rows √ó 31 columns\nNow, it‚Äôs as simple as‚Ä¶\nfrom llama_lime.explorer import Explorer\n\nexplorer = Explorer(cancer)\n\n/Users/chrisaddy/Library/Caches/pypoetry/virtualenvs/llama-lime-Ipq4cX_Y-py3.11/lib/python3.11/site-packages/numba/core/decorators.py:262: NumbaDeprecationWarning: numba.generated_jit is deprecated. Please see the documentation at: https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-generated-jit for more information and advice on a suitable replacement.\n  warnings.warn(msg, NumbaDeprecationWarning)\n/Users/chrisaddy/Library/Caches/pypoetry/virtualenvs/llama-lime-Ipq4cX_Y-py3.11/lib/python3.11/site-packages/visions/backends/shared/nan_handling.py:50: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  @nb.jit\n\n\n2023-08-18 15:26:48,163 INFO worker.py:1621 -- Started a local Ray instance.\nexplorer.explore(\"What column has the highest mean?\")\n\n{'question': 'What column has the highest mean?',\n 'chat_history': [HumanMessage(content='What column has the highest mean?', additional_kwargs={}, example=False),\n  AIMessage(content='The column with the highest mean is \"mean area\".', additional_kwargs={}, example=False)],\n 'answer': 'The column with the highest mean is \"mean area\".'}"
  },
  {
    "objectID": "exploring-data.html#back-and-forth-chatting",
    "href": "exploring-data.html#back-and-forth-chatting",
    "title": "Exploring Tabular Datasets",
    "section": "Back and Forth Chatting",
    "text": "Back and Forth Chatting\nThe Explorer, under the hood, has a memory buffer which we can use to have more natural back and forth chats.\nWithout the buffer, if we asked what is the minimum value of that column? after our initial prompt, it wouldn‚Äôt know to what that column refers. Now, since it has a memory buffer, it knows we are talking about the column with the highest mean.\n\nexplorer.explore(\"What is the minimum of that column\") \n\n{'question': 'What is the minimum of that column',\n 'chat_history': [HumanMessage(content='What column has the highest mean?', additional_kwargs={}, example=False),\n  AIMessage(content='The column with the highest mean is \"mean area\".', additional_kwargs={}, example=False),\n  HumanMessage(content='What is the minimum of that column', additional_kwargs={}, example=False),\n  AIMessage(content='The minimum value in the \"mean area\" column is 143.5.', additional_kwargs={}, example=False)],\n 'answer': 'The minimum value in the \"mean area\" column is 143.5.'}\n\n\n\nexplorer.explore(\"What are the top three highlights?\")\n\n{'question': 'What are the top three highlights?',\n 'chat_history': [HumanMessage(content='What column has the highest mean?', additional_kwargs={}, example=False),\n  AIMessage(content='The column with the highest mean is \"mean area\".', additional_kwargs={}, example=False),\n  HumanMessage(content='What is the minimum of that column', additional_kwargs={}, example=False),\n  AIMessage(content='The minimum value in the \"mean area\" column is 143.5.', additional_kwargs={}, example=False),\n  HumanMessage(content='What are the top three highlights?', additional_kwargs={}, example=False),\n  AIMessage(content='Based on the given context, the top three highlights are:\\n\\n1. The \"mean area\" column has a value of 569, indicating a high number of positive values.\\n2. The \"worst concave points\" column has a value of 556, indicating a high number of positive values.\\n3. The \"mean concave points\" column has a value of 556, indicating a high number of positive values.', additional_kwargs={}, example=False)],\n 'answer': 'Based on the given context, the top three highlights are:\\n\\n1. The \"mean area\" column has a value of 569, indicating a high number of positive values.\\n2. The \"worst concave points\" column has a value of 556, indicating a high number of positive values.\\n3. The \"mean concave points\" column has a value of 556, indicating a high number of positive values.'}"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "llama-lime",
    "section": "",
    "text": "llama-lime is a cutting-edge toolkit designed for data scientists, machine learning practitioners, researchers, and AI tinkerers. It provides powerful tools to analyze, explore, and explain complex models, including Large Language Models (LLMs), with transparency and ease."
  },
  {
    "objectID": "index.html#welcome-to-llama-lime-leveraging-large-models-for-analysis-and-local-interpretable-model-explanations",
    "href": "index.html#welcome-to-llama-lime-leveraging-large-models-for-analysis-and-local-interpretable-model-explanations",
    "title": "llama-lime",
    "section": "",
    "text": "llama-lime is a cutting-edge toolkit designed for data scientists, machine learning practitioners, researchers, and AI tinkerers. It provides powerful tools to analyze, explore, and explain complex models, including Large Language Models (LLMs), with transparency and ease."
  },
  {
    "objectID": "index.html#what-we-offer",
    "href": "index.html#what-we-offer",
    "title": "llama-lime",
    "section": "üéØ What We Offer",
    "text": "üéØ What We Offer\nExploration: Dive into your data with our Explorer class, equipped with comprehensive tools for Exploratory Data Analysis (EDA). Understand your data‚Äôs underlying patterns, distributions, and relationships like never before.\nExplanation: Demystify AI with our Explainer class, offering robust explainability techniques such as SHAP and LIME. From text to vision, we support various domains and tasks."
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "llama-lime",
    "section": "üåü Features",
    "text": "üåü Features\nVersatility: Seamlessly integrate llama-lime with diverse models, including Neural Networks, Random Forests, Hugging Face Transformers, and more.\nMultimodal Support: Analyze and explain models across a wide range of tasks, including classification, object detection, summarization, and more.\nUser-Centric: Enjoy llama-lime‚Äôs user-friendly and intuitive APIs, designed for both beginners and experts alike.\nCommunity-Driven: We value collaboration and innovation. Contribute, learn, and grow with the llama-lime community."
  },
  {
    "objectID": "index.html#how-to-get-started",
    "href": "index.html#how-to-get-started",
    "title": "llama-lime",
    "section": "üõ†Ô∏è How to Get Started",
    "text": "üõ†Ô∏è How to Get Started\nReady to explore the world of explainable AI? Check out our Getting Started Guide, Tutorials, and API Documentation to embark on your journey with llama-lime."
  },
  {
    "objectID": "index.html#join-the-community",
    "href": "index.html#join-the-community",
    "title": "llama-lime",
    "section": "ü§ù Join the Community",
    "text": "ü§ù Join the Community\nWe welcome contributions and collaboration. Whether you‚Äôre a seasoned professional or just starting in AI, there‚Äôs room for you in the llama-lime family. Visit our GitHub to connect, contribute, and innovate.\nüéâ Unlock the potential of AI with llama-lime. Explore, Explain, and Empower your models with transparency and confidence!"
  },
  {
    "objectID": "random_forest_explanation.html",
    "href": "random_forest_explanation.html",
    "title": "Explaining a Random Forest",
    "section": "",
    "text": "We‚Äôll start with a super easy example, the obligatory iris dataset with a random forest classifier. Below we set up the model using scikit-learn, everything so far should look very familiar.\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_iris\n\nnp.random.seed(123)\n\niris = load_iris()\nrandom_forest = RandomForestClassifier()\nSince we‚Äôll use GPT-4 for the tutorial, and because GPT-4 is very smart, it probably knows the iris dataset anyway. To trick it a little bit into thinking we have a new dataset, let‚Äôs just rename the features and ‚Äújumble‚Äù the data a bit.\nfeature_names = [\"length of table\", \"width of table\", \"length of dresser\", \"width of dresser\"]\nclass_names = [\"living room\", \"bedroom\", \"dining room\"]\n\nX = iris.data * 1.8182\ny = iris.target\nrandom_forest.fit(X, y)\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier()"
  },
  {
    "objectID": "random_forest_explanation.html#explaining-the-model",
    "href": "random_forest_explanation.html#explaining-the-model",
    "title": "Explaining a Random Forest",
    "section": "Explaining the Model",
    "text": "Explaining the Model\nFrom here, we‚Äôve fit the model and we can run predictions against the model with new data.\n\nimport numpy as np\n\nnew_observation = np.random.rand(1, 4)\n\nnew_observation\n\narray([[0.12062867, 0.8263408 , 0.60306013, 0.54506801]])\n\n\n\nrandom_forest.predict_proba(new_observation)\n\narray([[0.96, 0.04, 0.  ]])\n\n\nNow we can say that the most likely class to which our new observation belongs is living room. Of course, from here, we have a lot of questions. And if we don‚Äôt then the people we show our models to sure will üòÖ.\nThe random forest in scikit has some nice utilities for helping to diagnose what our model is doing under the hood. We can get the feature importances from the model.\n\nrandom_forest.feature_importances_\n\narray([0.08738947, 0.0257446 , 0.4611169 , 0.42574903])\n\n\nWe can even look at the decision path our new data took to reach it‚Äôs prediction.\n\nrandom_forest.decision_path(new_observation)\n\n(&lt;1x1708 sparse matrix of type '&lt;class 'numpy.int64'&gt;'\n    with 226 stored elements in Compressed Sparse Row format&gt;,\n array([   0,   17,   38,   59,   82,   97,  110,  123,  138,  157,  170,\n         183,  206,  225,  246,  269,  298,  323,  330,  347,  358,  375,\n         388,  407,  422,  447,  474,  493,  510,  525,  542,  549,  562,\n         581,  598,  611,  624,  641,  656,  675,  700,  715,  724,  747,\n         764,  777,  790,  803,  818,  835,  858,  873,  890,  911,  934,\n         949,  962,  977,  990, 1003, 1028, 1041, 1060, 1071, 1094, 1107,\n        1120, 1135, 1156, 1167, 1186, 1201, 1216, 1239, 1258, 1271, 1278,\n        1299, 1318, 1341, 1362, 1391, 1408, 1427, 1444, 1457, 1472, 1493,\n        1512, 1525, 1538, 1553, 1574, 1597, 1620, 1635, 1652, 1669, 1680,\n        1691, 1708]))\n\n\nGreat! We‚Äôre done!\n\nNot So Fast‚Ä¶\nSo, at this point there are a couple of issues.\n1.) What the heck does any of that mean? I‚Äôm a data scientist, and even I don‚Äôt think any of this output is useful. 2.) This works for a random forest. Does it work for anything else? Can I use it in any scikit model? What about outside of scikit? 3.) How do I make this make sense for a variety of different audiences? How do I explain it to my boss? The woman in accounting I‚Äôm building the model for? My mom?\nLet‚Äôs start with #2. There have been many advances in explainable machine learning in recent years, and a big focus has been on creating model-agnostic explainers. Two of the biggest ones are SHAP](https://shap.readthedocs.io/en/latest/tabular_examples.html) and LIME.\nSo, if we use model-agnostic explainers what do we get?\n\nimport shap\n\nshap_sample = shap.utils.sample(X, 10)\n\nexplainer = shap.Explainer(random_forest.predict, shap_sample)\nshap_values = explainer(X)\n\nUsing `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n\n\nThen we can even print out a nice plot!\n\nshap.plots.waterfall(shap_values[8], max_display=14)\n\n\n\n\nThis is great, but it‚Äôs still missing something. You still need to explain to a layperson what they‚Äôre looking at. This might help solve issue 1.) from above, but issue 3.) remains. We can make sense of the SHAP scores if they are displayed to us nicely, but we still need to know what the SHAP score is doing. Can we do better?"
  },
  {
    "objectID": "random_forest_explanation.html#creating-an-explainer-that-explains",
    "href": "random_forest_explanation.html#creating-an-explainer-that-explains",
    "title": "Explaining a Random Forest",
    "section": "Creating an Explainer that Explains",
    "text": "Creating an Explainer that Explains\nSHAP is great; we don‚Äôt want to redo the wheel. What we want to do is solve issue 3.) from above. To do so, let‚Äôs wrap up everything we have in a simple class, similar to the Explainer in the shap library.\n\nfrom llama_lime.explainer import Explainer\n\nexplainer = Explainer(\n    random_forest,\n    features=X,\n    feature_names=feature_names,\n    class_names=class_names,\n    output=y,\n    target_audience=\"a business person with a solid understanding of basic statistics\"\n    )\n\nYou might see that target_audience is something new. This takes in a natural-language prompt explaining\nJust like before we can print out the SHAP value.\n\nexplainer.add_shap(shap_sample)\n\nNow we HIT THE EXPLAIN BUTTON!.\n\nexplainer.explain()\n\nBased on the SHAP analysis, we can provide a detailed summary of the findings for the machine learning model. However, it seems that there are no specific results or findings available for this model. It is possible that the model did not provide any SHAP values or that the SHAP analysis was not conducted for this particular model.\n\nWithout any specific results, it is difficult to provide a detailed summary of the findings. It is important to note that SHAP values are useful in understanding the contribution of each feature towards the model's predictions. They help in explaining why the model made the predictions it did by quantifying the impact of each feature.\n\nIn a typical scenario, if we had the SHAP values, we could have identified the features that had the most significant positive or negative impact on the model's predictions. This information would have been valuable for a business person with a solid understanding of basic statistics, as it would have provided insights into which features are driving the model's predictions and how they are influencing the outcomes.\n\nHowever, since we do not have any specific results or findings, we cannot provide a detailed summary of the model's behavior or the reasons behind its predictions. It is recommended to further investigate the model or consult with a data scientist to obtain more insights into its behavior and predictions."
  },
  {
    "objectID": "random_forest_explanation.html#but-wait-theres-more",
    "href": "random_forest_explanation.html#but-wait-theres-more",
    "title": "Explaining a Random Forest",
    "section": "But Wait, There‚Äôs More‚Ä¶",
    "text": "But Wait, There‚Äôs More‚Ä¶\nWe can add in feature importances, class importances, feature-class interactions to provide even more context.\nSince this is based on the SHAP scores, these are model-agnostic versions of these measurements.\n\nexplainer.add_feature_importances()\nexplainer.add_class_importances()\nexplainer.add_feature_class_interactions()\n\nAgain, we just run explain() over the scores we are adding.\n\nexplainer.explain()\n\nBased on the analysis of the SHAP values, feature importance, class importances, and feature-class interactions, we did not find any significant findings or insights to report. The model's predictions do not appear to be strongly influenced by any specific features, classes, or interactions between features and classes. This suggests that the model's predictions are not heavily reliant on any particular factors and are likely a result of a combination of multiple features and classes.\n\nIt is important to note that these findings are specific to the dataset and model architecture used for this analysis. Different datasets or models may yield different results. Additionally, the absence of significant findings does not necessarily imply that the model is not performing well or that it is not capturing important patterns in the data. It simply means that the SHAP analysis did not reveal any specific features, classes, or interactions that have a strong influence on the model's predictions.\n\nFurther analysis and exploration may be required to gain a deeper understanding of the model's behavior and performance. This could include examining other evaluation metrics, conducting additional feature engineering, or exploring different model architectures."
  },
  {
    "objectID": "random_forest_explanation.html#with-extra-lime",
    "href": "random_forest_explanation.html#with-extra-lime",
    "title": "Explaining a Random Forest",
    "section": "With Extra Lime",
    "text": "With Extra Lime\nWe are also baking LIME into the explainability pie. This shouldn‚Äôt be too surprising given it was the inspiration for the name of the project.\n\nexplainer.add_lime(X[10:30])\nexplainer.explain()\n\nBased on the LIME instance explainer analysis, we have obtained feature contributions for each instance. These feature contributions indicate the impact of each feature on the prediction for a specific instance.\n\nThe output consists of a list of tuples, where each tuple contains a feature and its corresponding weight in the explanation. The first element of the tuple represents a statement about the feature's value, and the second element is the weight of that feature in the model's prediction.\n\nFor example, in the first tuple, the feature \"width of dresser\" with a value less than or equal to 0.55 has a negative weight of -0.195. This suggests that a smaller width of the dresser decreases the prediction. Similarly, the feature \"length of dresser\" with a value less than or equal to 2.91 has a negative weight of -0.176, indicating that a shorter length of the dresser also decreases the prediction.\n\nIt's important to note that the weights can be positive or negative, indicating whether a feature positively or negatively contributes to the prediction. The magnitude of the weight represents the strength of the feature's influence on the prediction.\n\nBy analyzing the feature contributions for each instance, we can gain insights into which features are most influential in determining the prediction for that instance. This information can be valuable for understanding the decision-making process of the model and identifying the key factors driving the predictions.\n\nPlease note that the specific findings and interpretations may vary depending on the actual values and features in the dataset."
  },
  {
    "objectID": "random_forest_explanation.html#one-last-trick",
    "href": "random_forest_explanation.html#one-last-trick",
    "title": "Explaining a Random Forest",
    "section": "One Last Trick",
    "text": "One Last Trick\nthe explain() method actually takes a query argument, which by default asks for a detailed summary of the context we‚Äôve given it.\nBut the explainer is just an LLMChain. In fact, most of this library is connecting existing methodologies to LangChain.\nSo, we are not confined to just summarizing the scores we give it. We can ask it any natural-language question.\n\nexplainer.explain(\"If we had to drop a single feature which one should it be and why?\")\n\nBased on the LIME instance explainer analysis, we can determine the feature to drop by examining the weights assigned to each feature in the explanation. The feature with the smallest absolute weight can be considered for removal, as it has the least impact on the model's prediction for the specific instance.\n\nIn the provided output, we can see that the features are listed along with their corresponding weights. For example, [('width of dresser &lt;= 0.55', -0.195), ('length of dresser &lt;= 2.91', -0.176), ('9.27 &lt; length of table &lt;= 10.55', 0.032), ('width of table &gt; 6.00', 0.012)].\n\nBased on these weights, we can observe that the feature '9.27 &lt; length of table &lt;= 10.55' has the smallest absolute weight of 0.032. Therefore, if we had to drop a single feature, we could consider dropping this feature.\n\nThe reason for dropping this feature is that it has the least impact on the model's prediction for the specific instance. By removing this feature, we can potentially simplify the model without significantly affecting its predictive performance.\n\nIt's important to note that this recommendation is specific to the given instance and may not generalize to other instances or the overall model. Additionally, further analysis and evaluation should be conducted to assess the impact of dropping this feature on the model's performance and interpretability.\n\n\n\nexplainer.explain(\"What is the most compelling story we can tell about this output?\")\n\nThe most compelling story we can tell about this output is that the width and length of the dresser, as well as the length and width of the table, have a significant impact on the model's predictions. \n\nThe feature importance analysis shows that the width of the dresser and the length of the dresser are consistently negatively correlated with the predictions. This suggests that smaller widths and lengths of the dresser tend to decrease the model's predictions. On the other hand, the width and length of the table have a mixed impact on the predictions, with some instances showing a positive correlation and others showing a negative correlation.\n\nThe class importances analysis indicates that the width and length of the dresser have a relatively higher influence on the predictions compared to other features. This suggests that these features play a more significant role in determining the outcome of the model's predictions.\n\nThe feature-class interaction analysis reveals that the impact of the features on the predictions varies across different classes. For example, the width of the dresser has a negative influence on the predictions for most classes, while the length of the table has a positive influence on the predictions for some classes.\n\nOverall, this analysis highlights the importance of considering the dimensions of the dresser and table when making predictions using this model. It suggests that smaller widths and lengths of the dresser tend to decrease the predictions, while the impact of the table dimensions is more nuanced and depends on the specific class being predicted.\n\n\n\nexplainer.explain(\"Should we pick a better model?\")\n\nBased on the provided analysis, there is not enough information to determine whether we should pick a better model. The analysis focused on explaining the predictions made by the model using different techniques such as SHAP values, feature importance, class importances, and feature-class interactions. However, no specific evaluation or comparison of the model's performance or accuracy was provided.\n\nTo determine whether we should pick a better model, we would need additional information such as the model's performance metrics (e.g., accuracy, precision, recall) and a comparison with other models or benchmarks. It is important to assess the model's performance and consider factors such as its predictive accuracy, interpretability, computational efficiency, and suitability for the specific business problem at hand.\n\nWithout this additional information, it is not possible to make a conclusive decision about whether a better model should be chosen."
  },
  {
    "objectID": "random_forest_explanation.html#conclusion",
    "href": "random_forest_explanation.html#conclusion",
    "title": "Explaining a Random Forest",
    "section": "Conclusion",
    "text": "Conclusion\nThis should serve as a general overview of what you can do with llama-lime. In further tutorials, we‚Äôll take a peek under the hood to look at how we can modify the explainer."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "nbeats_example.html",
    "href": "nbeats_example.html",
    "title": "llama-lime",
    "section": "",
    "text": "https://lightning-flash.readthedocs.io/en/latest/notebooks/flash_tutorials/electricity_forecasting.html"
  }
]