[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "llama-lime",
    "section": "",
    "text": "llama-lime is a cutting-edge toolkit designed for data scientists, machine learning practitioners, researchers, and AI tinkerers. It provides powerful tools to analyze, explore, and explain complex models, including Large Language Models (LLMs), with transparency and ease."
  },
  {
    "objectID": "index.html#welcome-to-llama-lime-leveraging-large-models-for-analysis-and-local-interpretable-model-explanations",
    "href": "index.html#welcome-to-llama-lime-leveraging-large-models-for-analysis-and-local-interpretable-model-explanations",
    "title": "llama-lime",
    "section": "",
    "text": "llama-lime is a cutting-edge toolkit designed for data scientists, machine learning practitioners, researchers, and AI tinkerers. It provides powerful tools to analyze, explore, and explain complex models, including Large Language Models (LLMs), with transparency and ease."
  },
  {
    "objectID": "index.html#what-we-offer",
    "href": "index.html#what-we-offer",
    "title": "llama-lime",
    "section": "üéØ What We Offer",
    "text": "üéØ What We Offer\nExploration: Dive into your data with our Explorer class, equipped with comprehensive tools for Exploratory Data Analysis (EDA). Understand your data‚Äôs underlying patterns, distributions, and relationships like never before.\nExplanation: Demystify AI with our Explainer class, offering robust explainability techniques such as SHAP and LIME. From text to vision, we support various domains and tasks."
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "llama-lime",
    "section": "üåü Features",
    "text": "üåü Features\nVersatility: Seamlessly integrate llama-lime with diverse models, including Neural Networks, Random Forests, Hugging Face Transformers, and more.\nMultimodal Support: Analyze and explain models across a wide range of tasks, including classification, object detection, summarization, and more.\nUser-Centric: Enjoy llama-lime‚Äôs user-friendly and intuitive APIs, designed for both beginners and experts alike.\nCommunity-Driven: We value collaboration and innovation. Contribute, learn, and grow with the llama-lime community."
  },
  {
    "objectID": "index.html#how-to-get-started",
    "href": "index.html#how-to-get-started",
    "title": "llama-lime",
    "section": "üõ†Ô∏è How to Get Started",
    "text": "üõ†Ô∏è How to Get Started\nReady to explore the world of explainable AI? Check out our Getting Started Guide, Tutorials, and API Documentation to embark on your journey with llama-lime."
  },
  {
    "objectID": "index.html#join-the-community",
    "href": "index.html#join-the-community",
    "title": "llama-lime",
    "section": "ü§ù Join the Community",
    "text": "ü§ù Join the Community\nWe welcome contributions and collaboration. Whether you‚Äôre a seasoned professional or just starting in AI, there‚Äôs room for you in the llama-lime family. Visit our GitHub to connect, contribute, and innovate.\nüéâ Unlock the potential of AI with llama-lime. Explore, Explain, and Empower your models with transparency and confidence!"
  },
  {
    "objectID": "random_forest_explanation.html",
    "href": "random_forest_explanation.html",
    "title": "Explaining a Random Forest",
    "section": "",
    "text": "We‚Äôll start with a super easy example, the obligatory iris dataset with a random forest classifier. Below we set up the model using scikit-learn, everything so far should look very familiar.\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_iris\n\nnp.random.seed(123)\n\niris = load_iris()\nrandom_forest = RandomForestClassifier()\nSince we‚Äôll use GPT-4 for the tutorial, and because GPT-4 is very smart, it probably knows the iris dataset anyway. To trick it a little bit into thinking we have a new dataset, let‚Äôs just rename the features and ‚Äújumble‚Äù the data a bit.\nfeature_names = [\"length of table\", \"width of table\", \"length of dresser\", \"width of dresser\"]\nclass_names = [\"living room\", \"bedroom\", \"dining room\"]\n\nX = iris.data * 1.8182\ny = iris.target\nrandom_forest.fit(X, y)\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier()"
  },
  {
    "objectID": "random_forest_explanation.html#explaining-the-model",
    "href": "random_forest_explanation.html#explaining-the-model",
    "title": "Explaining a Random Forest",
    "section": "Explaining the Model",
    "text": "Explaining the Model\nFrom here, we‚Äôve fit the model and we can run predictions against the model with new data.\n\nimport numpy as np\n\nnew_observation = np.random.rand(1, 4)\n\nnew_observation\n\narray([[0.12062867, 0.8263408 , 0.60306013, 0.54506801]])\n\n\n\nrandom_forest.predict_proba(new_observation)\n\narray([[0.96, 0.04, 0.  ]])\n\n\nNow we can say that the most likely class to which our new observation belongs is living room. Of course, from here, we have a lot of questions. And if we don‚Äôt then the people we show our models to sure will üòÖ.\nThe random forest in scikit has some nice utilities for helping to diagnose what our model is doing under the hood. We can get the feature importances from the model.\n\nrandom_forest.feature_importances_\n\narray([0.08738947, 0.0257446 , 0.4611169 , 0.42574903])\n\n\nWe can even look at the decision path our new data took to reach it‚Äôs prediction.\n\nrandom_forest.decision_path(new_observation)\n\n(&lt;1x1708 sparse matrix of type '&lt;class 'numpy.int64'&gt;'\n    with 226 stored elements in Compressed Sparse Row format&gt;,\n array([   0,   17,   38,   59,   82,   97,  110,  123,  138,  157,  170,\n         183,  206,  225,  246,  269,  298,  323,  330,  347,  358,  375,\n         388,  407,  422,  447,  474,  493,  510,  525,  542,  549,  562,\n         581,  598,  611,  624,  641,  656,  675,  700,  715,  724,  747,\n         764,  777,  790,  803,  818,  835,  858,  873,  890,  911,  934,\n         949,  962,  977,  990, 1003, 1028, 1041, 1060, 1071, 1094, 1107,\n        1120, 1135, 1156, 1167, 1186, 1201, 1216, 1239, 1258, 1271, 1278,\n        1299, 1318, 1341, 1362, 1391, 1408, 1427, 1444, 1457, 1472, 1493,\n        1512, 1525, 1538, 1553, 1574, 1597, 1620, 1635, 1652, 1669, 1680,\n        1691, 1708]))\n\n\nGreat! We‚Äôre done!\n\nNot So Fast‚Ä¶\nSo, at this point there are a couple of issues.\n1.) What the heck does any of that mean? I‚Äôm a data scientist, and even I don‚Äôt think any of this output is useful. 2.) This works for a random forest. Does it work for anything else? Can I use it in any scikit model? What about outside of scikit? 3.) How do I make this make sense for a variety of different audiences? How do I explain it to my boss? The woman in accounting I‚Äôm building the model for? My mom?\nLet‚Äôs start with #2. There have been many advances in explainable machine learning in recent years, and a big focus has been on creating model-agnostic explainers. Two of the biggest ones are SHAP](https://shap.readthedocs.io/en/latest/tabular_examples.html) and LIME.\nSo, if we use model-agnostic explainers what do we get?\n\nimport shap\n\nshap_sample = shap.utils.sample(X, 10)\n\nexplainer = shap.Explainer(random_forest.predict, shap_sample)\nshap_values = explainer(X)\n\nIProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n\n\nThen we can even print out a nice plot!\n\nshap.plots.waterfall(shap_values[8], max_display=14)\n\n\n\n\nThis is great, but it‚Äôs still missing something. You still need to explain to a layperson what they‚Äôre looking at. This might help solve issue 1.) from above, but issue 3.) remains. We can make sense of the SHAP scores if they are displayed to us nicely, but we still need to know what the SHAP score is doing. Can we do better?"
  },
  {
    "objectID": "random_forest_explanation.html#creating-an-explainer-that-explains",
    "href": "random_forest_explanation.html#creating-an-explainer-that-explains",
    "title": "Explaining a Random Forest",
    "section": "Creating an Explainer that Explains",
    "text": "Creating an Explainer that Explains\nSHAP is great; we don‚Äôt want to redo the wheel. What we want to do is solve issue 3.) from above. To do so, let‚Äôs wrap up everything we have in a simple class, similar to the Explainer in the shap library.\n\nfrom llama_lime.explainer import Explainer\n\nexplainer = Explainer(\n    random_forest,\n    features=X,\n    feature_names=feature_names,\n    class_names=class_names,\n    output=y,\n    target_audience=\"a business person with a solid understanding of basic statistics\"\n    )\n\nYou might see that target_audience is something new. This takes in a natural-language prompt explaining\nJust like before we can print out the SHAP value.\n\nexplainer.add_shap(shap_sample)\n\n\nexplainer.explain()\n\nBased on the SHAP analysis, we can provide a detailed summary of the findings for the machine learning model. However, it seems that there are no specific results or findings available for this model. It is possible that the model did not provide any SHAP values or that the SHAP analysis was not conducted for this particular model.\n\nWithout any specific results, it is difficult to provide a detailed summary of the findings. It is important to note that SHAP values are useful in understanding the contribution of each feature towards the model's predictions. They provide insights into how each feature affects the model's output and can help in interpreting the model's decision-making process.\n\nIf there were specific results available, we could have provided a summary of the findings by analyzing the SHAP values for each feature. We could have identified the features that had the most significant positive or negative impact on the model's predictions. This information would have been valuable in understanding the factors driving the model's decisions and potentially identifying areas for improvement or further investigation.\n\nHowever, since there are no specific results available, we cannot provide a detailed summary of the findings for this model. It is recommended to conduct the SHAP analysis again or explore other interpretability techniques to gain insights into the model's predictions.\n\n\n\nexplainer.add_feature_importances()\nexplainer.add_class_importances()\nexplainer.add_instance_importances()\nexplainer.add_feature_class_interactions()\n\n\nexplainer.explain()\n\nBased on the analysis of the SHAP values, feature importance, class importances, instance importances, and feature-class interactions, we have not found any significant findings or results to report. It is possible that the model did not identify any strong relationships or patterns between the features and the target variable. This could indicate that the features may not have a significant impact on the predictions made by the model, or that the model may not be well-suited for capturing the relationships between the features and the target variable.\n\nIt is important to note that the absence of significant findings does not necessarily mean that the model is not performing well or that it is not useful. It simply suggests that the features may not have a strong influence on the predictions made by the model. It is also possible that the model is making predictions based on other factors or features that were not included in the analysis.\n\nFurther investigation and analysis may be required to gain a deeper understanding of the model's performance and the factors that contribute to its predictions. This could involve exploring additional features, refining the model architecture, or considering alternative modeling approaches.\n\n\n\nexplainer.add_lime(X[10:30])\nexplanations = explainer.explain()\n\nBased on the LIME instance explainer analysis, we have obtained feature contributions for each instance. These feature contributions indicate the impact of each feature on the prediction for a specific instance.\n\nThe output consists of a list of tuples, where each tuple contains a feature and its corresponding weight in the explanation. The first element of the tuple represents a statement about the feature's value, and the second element is the weight of that feature in the model's prediction.\n\nFor example, in the first tuple, the feature \"width of dresser\" with a value less than or equal to 0.55 has a negative weight of -0.195. This suggests that a smaller width of the dresser decreases the model's prediction. Similarly, the feature \"length of dresser\" with a value less than or equal to 2.91 has a negative weight of -0.176, indicating that a shorter length of the dresser also decreases the model's prediction.\n\nIt's important to note that the weights can be positive or negative, indicating whether a feature positively or negatively contributes to the prediction. The magnitude of the weight represents the strength of the feature's influence on the prediction.\n\nBy analyzing the feature contributions for each instance, we can gain insights into which features are driving the model's predictions and understand the relationship between the features and the predicted outcome. This information can be valuable for understanding the decision-making process of the model and potentially identifying areas for improvement or further investigation.\n\nPlease note that the specific findings and interpretations may vary depending on the actual values and context of the features in your dataset.\n\n\n\nexplainer.explain()\n\nBased on the analysis using SHAP values, feature importance, class importances, instance importances, and feature-class interactions, we have gained insights into the model's predictions and the factors that contribute to those predictions.\n\nUnfortunately, the analysis did not provide any specific results or findings. The output provided does not contain any information about the feature importance, class importances, instance importances, or feature-class interactions. Therefore, we cannot provide a detailed summary of the findings based on this analysis.\n\nIt is important to note that the absence of results could be due to various reasons, such as the model not having any significant feature importance, class importances, or instance importances. It is also possible that the analysis was not conducted properly or that the data used for the analysis was not suitable for generating meaningful insights.\n\nTo gain a better understanding of the model's predictions and the factors influencing them, it may be necessary to conduct further analysis or explore alternative methods for interpreting the model, such as LIME (Local Interpretable Model-agnostic Explanations). LIME can provide explanations at the instance level by creating a local surrogate model and identifying the feature contributions for specific instances. This can help in understanding the model's behavior and decision-making process for individual predictions.\n\nIn summary, based on the current analysis, we do not have any specific findings or insights to share. Further investigation or alternative methods may be required to gain a deeper understanding of the model's predictions.\n\n\n\nexplainer.add_feature_importances()\nexplainer.add_class_importances()\nexplainer.add_instance_importances()\nexplainer.add_feature_class_interactions()\nexplainer.add_lime(X[10:12])\n\n\nexplainer.explain()\n\nBased on the analysis, we have explored different methods for explaining the predictions of a machine learning model. \n\nFirst, we looked at the feature importance using SHAP values. The normalized feature importance measures the relative importance of each feature by considering the absolute contribution of each feature across all instances and classes. However, in this analysis, we did not find any specific results for feature importance.\n\nNext, we examined the class importances, which provide insights into how each class is influenced by the different features. Unfortunately, we did not find any specific results for class importances in this analysis.\n\nWe also explored instance importances, which measure how much each individual instance is influenced by the features in the model. However, we did not find any specific results for instance importances in this analysis.\n\nLastly, we discussed the LIME instance explainer, which is a model-agnostic method for explaining predictions. LIME creates a local surrogate model around a specific instance and perturbs the instance to create modified versions. The original model is then used to predict the outcomes of these modified instances, and a local surrogate model is fitted to interpret the prediction for the original instance. The explanations provided by LIME are in the form of feature contributions, represented as weights or coefficients in the local surrogate model. However, the specific output format and results for LIME were not provided in this analysis.\n\nIn summary, while we have explored different methods for explaining predictions, the analysis did not provide specific findings or results for feature importance, class importances, instance importances, or LIME explanations."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "nbeats_example.html",
    "href": "nbeats_example.html",
    "title": "llama-lime",
    "section": "",
    "text": "https://lightning-flash.readthedocs.io/en/latest/notebooks/flash_tutorials/electricity_forecasting.html"
  }
]