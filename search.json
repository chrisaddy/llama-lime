[
  {
    "objectID": "exploring-data.html",
    "href": "exploring-data.html",
    "title": "Exploring Tabular Datasets",
    "section": "",
    "text": "A pretty common task in data science is exploratory data analysis. Let‚Äôs do that. We‚Äôll use the breast cancer dataset from scikit-learn.\nfrom sklearn import datasets\nimport pandas as pd\n\ndef load_cancer_dataset():\n    dataset = datasets.load_breast_cancer()\n    dataframe = pd.DataFrame(dataset[\"data\"])\n    dataframe.columns = dataset[\"feature_names\"]\n    dataframe[\"status\"] = dataset.target\n    dataframe[\"status\"] = dataframe[\"status\"].replace({0: \"malignant\", 1: \"benign\"})\n\n    return dataframe\n\ncancer = load_cancer_dataset()\n\ncancer\nNow, it‚Äôs as simple as‚Ä¶\nexplorer = Explorer(cancer)\nexplorer.explore(\"What column has the highest mean?\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "llama-lime",
    "section": "",
    "text": "llama-lime is a cutting-edge toolkit designed for data scientists, machine learning practitioners, researchers, and AI tinkerers. It provides powerful tools to analyze, explore, and explain complex models, including Large Language Models (LLMs), with transparency and ease."
  },
  {
    "objectID": "index.html#welcome-to-llama-lime-leveraging-large-models-for-analysis-and-local-interpretable-model-explanations",
    "href": "index.html#welcome-to-llama-lime-leveraging-large-models-for-analysis-and-local-interpretable-model-explanations",
    "title": "llama-lime",
    "section": "",
    "text": "llama-lime is a cutting-edge toolkit designed for data scientists, machine learning practitioners, researchers, and AI tinkerers. It provides powerful tools to analyze, explore, and explain complex models, including Large Language Models (LLMs), with transparency and ease."
  },
  {
    "objectID": "index.html#what-we-offer",
    "href": "index.html#what-we-offer",
    "title": "llama-lime",
    "section": "üéØ What We Offer",
    "text": "üéØ What We Offer\nExploration: Dive into your data with our Explorer class, equipped with comprehensive tools for Exploratory Data Analysis (EDA). Understand your data‚Äôs underlying patterns, distributions, and relationships like never before.\nExplanation: Demystify AI with our Explainer class, offering robust explainability techniques such as SHAP and LIME. From text to vision, we support various domains and tasks."
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "llama-lime",
    "section": "üåü Features",
    "text": "üåü Features\nVersatility: Seamlessly integrate llama-lime with diverse models, including Neural Networks, Random Forests, Hugging Face Transformers, and more.\nMultimodal Support: Analyze and explain models across a wide range of tasks, including classification, object detection, summarization, and more.\nUser-Centric: Enjoy llama-lime‚Äôs user-friendly and intuitive APIs, designed for both beginners and experts alike.\nCommunity-Driven: We value collaboration and innovation. Contribute, learn, and grow with the llama-lime community."
  },
  {
    "objectID": "index.html#how-to-get-started",
    "href": "index.html#how-to-get-started",
    "title": "llama-lime",
    "section": "üõ†Ô∏è How to Get Started",
    "text": "üõ†Ô∏è How to Get Started\nReady to explore the world of explainable AI? Check out our Getting Started Guide, Tutorials, and API Documentation to embark on your journey with llama-lime."
  },
  {
    "objectID": "index.html#join-the-community",
    "href": "index.html#join-the-community",
    "title": "llama-lime",
    "section": "ü§ù Join the Community",
    "text": "ü§ù Join the Community\nWe welcome contributions and collaboration. Whether you‚Äôre a seasoned professional or just starting in AI, there‚Äôs room for you in the llama-lime family. Visit our GitHub to connect, contribute, and innovate.\nüéâ Unlock the potential of AI with llama-lime. Explore, Explain, and Empower your models with transparency and confidence!"
  },
  {
    "objectID": "random_forest_explanation.html",
    "href": "random_forest_explanation.html",
    "title": "Explaining a Random Forest",
    "section": "",
    "text": "We‚Äôll start with a super easy example, the obligatory iris dataset with a random forest classifier. Below we set up the model using scikit-learn, everything so far should look very familiar.\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_iris\n\nnp.random.seed(123)\n\niris = load_iris()\nrandom_forest = RandomForestClassifier()\nSince we‚Äôll use GPT-4 for the tutorial, and because GPT-4 is very smart, it probably knows the iris dataset anyway. To trick it a little bit into thinking we have a new dataset, let‚Äôs just rename the features and ‚Äújumble‚Äù the data a bit.\nfeature_names = [\"length of table\", \"width of table\", \"length of dresser\", \"width of dresser\"]\nclass_names = [\"living room\", \"bedroom\", \"dining room\"]\n\nX = iris.data * 1.8182\ny = iris.target\nrandom_forest.fit(X, y)\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier()"
  },
  {
    "objectID": "random_forest_explanation.html#explaining-the-model",
    "href": "random_forest_explanation.html#explaining-the-model",
    "title": "Explaining a Random Forest",
    "section": "Explaining the Model",
    "text": "Explaining the Model\nFrom here, we‚Äôve fit the model and we can run predictions against the model with new data.\n\nimport numpy as np\n\nnew_observation = np.random.rand(1, 4)\n\nnew_observation\n\narray([[0.12062867, 0.8263408 , 0.60306013, 0.54506801]])\n\n\n\nrandom_forest.predict_proba(new_observation)\n\narray([[0.96, 0.04, 0.  ]])\n\n\nNow we can say that the most likely class to which our new observation belongs is living room. Of course, from here, we have a lot of questions. And if we don‚Äôt then the people we show our models to sure will üòÖ.\nThe random forest in scikit has some nice utilities for helping to diagnose what our model is doing under the hood. We can get the feature importances from the model.\n\nrandom_forest.feature_importances_\n\narray([0.08738947, 0.0257446 , 0.4611169 , 0.42574903])\n\n\nWe can even look at the decision path our new data took to reach it‚Äôs prediction.\n\nrandom_forest.decision_path(new_observation)\n\n(&lt;1x1708 sparse matrix of type '&lt;class 'numpy.int64'&gt;'\n    with 226 stored elements in Compressed Sparse Row format&gt;,\n array([   0,   17,   38,   59,   82,   97,  110,  123,  138,  157,  170,\n         183,  206,  225,  246,  269,  298,  323,  330,  347,  358,  375,\n         388,  407,  422,  447,  474,  493,  510,  525,  542,  549,  562,\n         581,  598,  611,  624,  641,  656,  675,  700,  715,  724,  747,\n         764,  777,  790,  803,  818,  835,  858,  873,  890,  911,  934,\n         949,  962,  977,  990, 1003, 1028, 1041, 1060, 1071, 1094, 1107,\n        1120, 1135, 1156, 1167, 1186, 1201, 1216, 1239, 1258, 1271, 1278,\n        1299, 1318, 1341, 1362, 1391, 1408, 1427, 1444, 1457, 1472, 1493,\n        1512, 1525, 1538, 1553, 1574, 1597, 1620, 1635, 1652, 1669, 1680,\n        1691, 1708]))\n\n\nGreat! We‚Äôre done!\n\nNot So Fast‚Ä¶\nSo, at this point there are a couple of issues.\n1.) What the heck does any of that mean? I‚Äôm a data scientist, and even I don‚Äôt think any of this output is useful. 2.) This works for a random forest. Does it work for anything else? Can I use it in any scikit model? What about outside of scikit? 3.) How do I make this make sense for a variety of different audiences? How do I explain it to my boss? The woman in accounting I‚Äôm building the model for? My mom?\nLet‚Äôs start with #2. There have been many advances in explainable machine learning in recent years, and a big focus has been on creating model-agnostic explainers. Two of the biggest ones are SHAP](https://shap.readthedocs.io/en/latest/tabular_examples.html) and LIME.\nSo, if we use model-agnostic explainers what do we get?\n\nimport shap\n\nshap_sample = shap.utils.sample(X, 10)\n\nexplainer = shap.Explainer(random_forest.predict, shap_sample)\nshap_values = explainer(X)\n\nUsing `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n\n\nThen we can even print out a nice plot!\n\nshap.plots.waterfall(shap_values[8], max_display=14)\n\n\n\n\nThis is great, but it‚Äôs still missing something. You still need to explain to a layperson what they‚Äôre looking at. This might help solve issue 1.) from above, but issue 3.) remains. We can make sense of the SHAP scores if they are displayed to us nicely, but we still need to know what the SHAP score is doing. Can we do better?"
  },
  {
    "objectID": "random_forest_explanation.html#creating-an-explainer-that-explains",
    "href": "random_forest_explanation.html#creating-an-explainer-that-explains",
    "title": "Explaining a Random Forest",
    "section": "Creating an Explainer that Explains",
    "text": "Creating an Explainer that Explains\nSHAP is great; we don‚Äôt want to redo the wheel. What we want to do is solve issue 3.) from above. To do so, let‚Äôs wrap up everything we have in a simple class, similar to the Explainer in the shap library.\n\nfrom llama_lime.explainer import Explainer\n\nexplainer = Explainer(\n    random_forest,\n    features=X,\n    feature_names=feature_names,\n    class_names=class_names,\n    output=y,\n    target_audience=\"a business person with a solid understanding of basic statistics\"\n    )\n\nYou might see that target_audience is something new. This takes in a natural-language prompt explaining\nJust like before we can print out the SHAP value.\n\nexplainer.add_shap(shap_sample)\n\nNow we HIT THE EXPLAIN BUTTON!.\n\nexplainer.explain()\n\nBased on the SHAP analysis, we can provide a detailed summary of the findings for the machine learning model. However, it seems that there are no specific results or findings available for this model. It is possible that the model did not provide any SHAP values or that the SHAP analysis was not conducted for this particular model.\n\nWithout any specific results, it is difficult to provide a detailed summary of the findings. It is important to note that SHAP values are useful in understanding the contribution of each feature towards the model's predictions. They help in explaining why the model made the predictions it did by quantifying the impact of each feature.\n\nIn a typical scenario, if we had the SHAP values, we could have identified the features that had the most significant positive or negative impact on the model's predictions. This information would have been valuable for a business person with a solid understanding of basic statistics, as it would have provided insights into which features are driving the model's predictions and how they are influencing the outcomes.\n\nHowever, since we do not have any specific results or findings, we cannot provide a detailed summary of the model's behavior or the reasons behind its predictions. It is recommended to further investigate the model or consult with a data scientist to obtain more insights into its behavior and predictions."
  },
  {
    "objectID": "random_forest_explanation.html#but-wait-theres-more",
    "href": "random_forest_explanation.html#but-wait-theres-more",
    "title": "Explaining a Random Forest",
    "section": "But Wait, There‚Äôs More‚Ä¶",
    "text": "But Wait, There‚Äôs More‚Ä¶\nWe can add in feature importances, class importances, feature-class interactions to provide even more context.\nSince this is based on the SHAP scores, these are model-agnostic versions of these measurements.\n\nexplainer.add_feature_importances()\nexplainer.add_class_importances()\nexplainer.add_feature_class_interactions()\n\nAgain, we just run explain() over the scores we are adding.\n\nexplainer.explain()\n\nBased on the analysis of the SHAP values, feature importance, class importances, and feature-class interactions, we did not find any significant findings or insights to report. The model's predictions do not appear to be strongly influenced by any specific features, classes, or interactions between features and classes. This suggests that the model's predictions are not heavily reliant on any particular factors and are likely a result of a combination of multiple features and classes.\n\nIt is important to note that these findings are specific to the dataset and model architecture used for this analysis. Different datasets or models may yield different results. Additionally, the absence of significant findings does not necessarily imply that the model is not performing well or that it is not capturing important patterns in the data. It simply means that the SHAP analysis did not reveal any specific features, classes, or interactions that have a strong influence on the model's predictions."
  },
  {
    "objectID": "random_forest_explanation.html#with-extra-lime",
    "href": "random_forest_explanation.html#with-extra-lime",
    "title": "Explaining a Random Forest",
    "section": "With Extra Lime",
    "text": "With Extra Lime\nWe are also baking LIME into the explainability pie. This shouldn‚Äôt be too surprising given it was the inspiration for the name of the project.\n\nexplainer.add_lime(X[10:30])\nexplainer.explain()\n\nBased on the LIME instance explainer analysis, we have obtained feature contributions for each instance. These feature contributions indicate the impact of each feature on the prediction for a specific instance.\n\nThe output consists of a list of tuples, where each tuple contains a feature and its corresponding weight in the explanation. The first element of the tuple represents a statement about the feature's value, and the second element is the weight of that feature in the model's prediction.\n\nFor example, in the first tuple, the feature \"width of dresser\" with a value less than or equal to 0.55 has a negative weight of -0.195. This suggests that a smaller width of the dresser decreases the prediction. Similarly, the feature \"length of dresser\" with a value less than or equal to 2.91 has a negative weight of -0.176, indicating that a shorter length of the dresser also decreases the prediction.\n\nIt is important to note that the weights are relative to the specific instance being explained. Therefore, these feature contributions provide insights into how each feature influenced the prediction for that particular instance.\n\nBy analyzing the feature contributions across multiple instances, we can gain a deeper understanding of how different features impact the model's predictions. This information can be valuable for business decision-making, as it allows us to identify the key factors driving the model's predictions and potentially uncover any biases or patterns in the data.\n\nHowever, without specific numerical values or a target variable, it is difficult to provide a more detailed summary of the findings."
  },
  {
    "objectID": "random_forest_explanation.html#one-last-trick",
    "href": "random_forest_explanation.html#one-last-trick",
    "title": "Explaining a Random Forest",
    "section": "One Last Trick",
    "text": "One Last Trick\nthe explain() method actually takes a query argument, which by default asks for a detailed summary of the context we‚Äôve given it.\nBut the explainer is just an LLMChain. In fact, most of this library is connecting existing methodologies to LangChain.\nSo, we are not confined to just summarizing the scores we give it. We can ask it any natural-language question.\n\nexplainer.explain(\"If we had to drop a single feature which one should it be and why?\")\n\nBased on the LIME instance explainer analysis, we can determine the feature to drop by examining the weights assigned to each feature in the explanation. The feature with the smallest absolute weight can be considered for removal, as it has the least impact on the model's prediction for the specific instance.\n\nIn the provided output, we can see that the features are listed along with their corresponding weights. For example, [('width of dresser &lt;= 0.55', -0.195), ('length of dresser &lt;= 2.91', -0.176), ('9.27 &lt; length of table &lt;= 10.55', 0.032), ('width of table &gt; 6.00', 0.012)].\n\nBased on these weights, we can observe that the feature '9.27 &lt; length of table &lt;= 10.55' has the smallest absolute weight of 0.032. Therefore, if we had to drop a single feature, we could consider dropping this feature.\n\nThe reason for dropping this feature is that it has the least impact on the model's prediction for the specific instance. By removing this feature, we can potentially simplify the model without significantly affecting its predictive performance.\n\nIt's important to note that this recommendation is specific to the given instance and may not generalize to other instances or the overall model. Additionally, further analysis and evaluation should be conducted to assess the impact of dropping this feature on the model's performance and interpretability.\n\n\n\nexplainer.explain(\"What is the most compelling story we can tell about this output?\")\n\nThe most compelling story we can tell about this output is that the width and length of the dresser, as well as the length and width of the table, have a significant impact on the model's predictions. \n\nThe feature importance analysis shows that the width of the dresser and the length of the dresser are consistently negatively correlated with the predictions. This suggests that smaller widths and lengths of the dresser tend to decrease the model's predictions. On the other hand, the width and length of the table have a mixed impact on the predictions, with some instances showing a positive correlation and others showing a negative correlation.\n\nThe class importances analysis indicates that the width and length of the dresser have a relatively higher influence on the predictions compared to other features. This suggests that these features play a more significant role in determining the outcome of the model's predictions.\n\nThe feature-class interaction analysis reveals that the impact of the features on the predictions varies across different classes. For example, the width of the dresser has a negative influence on the predictions for most classes, while the length of the table has a positive influence on the predictions for some classes.\n\nOverall, this analysis highlights the importance of considering the dimensions of the dresser and table when making predictions. It suggests that smaller widths and lengths of the dresser tend to decrease the predictions, while the impact of the table dimensions varies depending on the specific class. This information can be valuable for understanding the factors that contribute to the model's predictions and making informed decisions based on these insights.\n\n\n\nexplainer.explain(\"Should we pick a better model?\")\n\nBased on the provided analysis, there is no specific information or results available to determine whether we should pick a better model. The analysis focused on explaining the predictions made by the model using different techniques such as SHAP values, feature importance, class importances, and feature-class interactions. However, no specific findings or conclusions were mentioned in the analysis.\n\nTo make a decision on whether to pick a better model, we would need additional information such as the performance metrics of the current model, comparison with other models, or any specific issues or limitations identified with the current model. Without this information, it is not possible to determine if a better model should be chosen."
  },
  {
    "objectID": "random_forest_explanation.html#conclusion",
    "href": "random_forest_explanation.html#conclusion",
    "title": "Explaining a Random Forest",
    "section": "Conclusion",
    "text": "Conclusion\nThis should serve as a general overview of what you can do with llama-lime. In further tutorials, we‚Äôll take a peek under the hood to look at how we can modify the explainer."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "nbeats_example.html",
    "href": "nbeats_example.html",
    "title": "llama-lime",
    "section": "",
    "text": "https://lightning-flash.readthedocs.io/en/latest/notebooks/flash_tutorials/electricity_forecasting.html"
  }
]